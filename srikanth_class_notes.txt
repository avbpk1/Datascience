-- Start 23 Jun

Gradient descent is an optimisation technique

LEarning Rate -- is the incremental step used to optimize the descent 

--price_finder_models.ipynb
eta0 -- hyper parameter 
max_iterations == 1000
tol=0.001 -- when slope is smaller than this or the iterations 1000 which ever reaches first, will stop

---------Regularisation is important --------
2 important terms before that
a) Bias b) Variance
Bias is bascially error with training data -- when model doesnot capture true relationship in train data -- it is called bias
LowBias is overfitting i.e. where model understood train data well. 
When model is overfitting , it could result in high variance.

goal is to have low bias and ow variance -- balance is important

when a model is underfitting -- increase the bias so that variance can go down.
How to increase bias -- Regularization is the technique
Regularizes coefficients/weights/slopes/model parameters to avoid overfitting. 

When slopes are high -- more complex model. When slopes are reduced. Models will perform better


High Bias is where model is generatlized to training data and might work well with new data
005.png

coefficients are to be decreased by a little

L1 and L2  (lasso and ridge)
increase bias and reduce variance

Lasso Regularization
L1 allows feature selection 
Lambda is the tuning factor
regularization_demo -- Lambdda iin the forumal is alpha
006.png -- train data fits well but not test data
007.png -- difference between train data and test data is lessened. 
008.png -- original line and regularized line. Goal is to get the regularized line


To achieve that
Ridge Regression can be used

from sklearn.linear_model import Ridge
ridge.fit(X_train, y_train)

ridge = Ridge(norlaize=True,alpha=1.0)
ridge.fit(X_train,y_train) -- alpah can be modified to reduce the sloppe

---LassoCV
from sklearn.linear_model import LassoCV

lasso = LassoCV(normalize = True,  cv=3,alphase = [2,3,4]
lasso.fit(X_train,y_train)
lasso.coef_
lasso.alpha

-- 2 different techniques


---------Grid Search -----------------
Learning algorithm 

Lets say h1, h2, h3 are hyper parameters -- 009.png

The green colour at the top -- lets say that is the best

---gridesearch_ref.ipynb
model = RandomForestRegressor()
p


Grid Search is the best way to determine which is the best model

GridSearchCV -- it is called GridSearchCrossValidation

Model can be any thing -- like gradient descent -- hyper parameters must relate to the model


complexity can be reduced by regularization. One of the ways is also to reduce the number of features. Dimensionality/feature reduction is a topic. If it is not possible to do feature reduction, regularization is a technique



---------Pipeline-------------






-- End 23rd June

-- Start 22nd Jun
prepare_data.ipynb-- for preparing car data -- pricing for cars
price_finder_models.ipynb

dummies or one_hot encoding will increase the columns

-- Linear Regression -- to start with
80 - 2- and build it
find coefficients of the model -- coefficenets are nothing but slopes in linear regression. For every unit of increase in size, there is 36 units. For e.g. if there is an increment of 1 size  in engine size -- there is 36 dollars increase in size

exercise -- standardise the data and try

Random forest regressor -- try it
feature importance can be derived using gini index

Gradient Descent and Stocastic Gradient descent

Gradient descent is an optimisation technique for parameters. Slope/Weight/Coefficient or Model Parameters all mean the same. These are not hyper parameters
Reduce Cost -- Cost here means error -- i.e. difference between predicted and actual -- goal is to reduce the cost. At what slope do we get the best cost.
In matematical terms , slope is called partial derivative
Algorithm is designed in such a way that the slope is the best possible one -- 
It optimises the intercept anbd slope to reduce cost
Cost function is always a convex function -- like a bowl 

-- End 22nd June

-- Start 20 Jun-- 
Tokenize, count, normalize -- tfidf

TfidfVectorizer -- 2 steps together -- convert a collection of raw documents to a matrix of TF-IDF features

hotel_reviews
more frequency , less importance

tfidf.vocabulary_

corpus should be bigger

Assignment -- find out where the token is not 0 -- then it is
find out which words it is considering negative or positive

Algorithm was Logistic regression


Text Classification using BernoulliNB
Variation of NaiveBayes
stop wrods are not given i.e. common english words are not ignore
Labelling data is important for supervised classification

if no labelling is done -- then it can be done by unsupervised learning

try gradient boost, grid search

loan -- data wrangling
admissions -- no data wrangling
reviews -- text analysis

car -- case study -- price of the car based on details -- uci machine mearning repository -- view all data sets
car/prepare_data.ipynb

-- End 20 Jun



-- STart 19Jun
stackingclassifier
Base learners are preditors/classifier -- same terminology
page 75 if meachine learning

MetaLearner is converged from base learners

Baselearners can be different algoruithms -- for e,g, Decistion Tree, KNN, SVC

3 Ensemble Methods

Boosting, Bagging, Stacking -- remember boosting is sequential and weighted
all work on the principle of wisdom of the crowd
they are all based on the priniciple that a combination of simple prediction models 


--> How to DEal with Cross Validation
92.png -- validation techniques also 93.png

A part of the available test is used for training and a part of it used for testing
-- problem is only 80% is used for training and 20% for testing
Another technique is
K-Folds -- 94.png

K-Fold for Cross Validation -- with this technique, we are effectiviely using entire data for training and testing. Especially when there is less amount of data.

Others are LOCF -- Leave out one for cross validation

Stratified K Fold Cross Validation -- is a varaiation of K Fold Cross validation to ensure that there is a good representation of data when compared to simple K Fold Cross Validation which has a tendency to not having good representation of data

-- When dealing with cross validation we dont specifiy percentage of train/test -- it internally decides -- cross_validation.ipynb

cross_val_score is the name of the function

Logistic Regression with built in cross validation 

Algorithm MSC -- Accuracy

----- Next Case Study -- Sentiment Analysis

Labelling is an important aspect in reviews -- for sentiment analysis for example
It is a key exercise and could be manual


-- For models to be built -- text has to be convert to numbers because the models work on numeric data
--bag_of_words.ipynb
This called bag of words
corpus -- refers to data
Steps required are

1) Tokenize -- Tokenization is the process of splitting words from text. 
2) Counting -- Occurance if the words 
3) Normalizing -- Normalizing and weighting with diminshing importance tokens that occur in the majority of the samples/documents

Bag of Words
We call vectorization i.e. Tokenize, Count and Normalize) the genreral process of turning a collection of text documents into numerical feature vectors

The specific strategy is called Bag of Words or Bag of n-grams

count vetcorizer
sklearn.featureextractor from text

Each Token is treated as a column

95.png, 96.png-- explains about the complexity of tokenization
scipy.sparse.csr.csr_matrix 
Sparse matrix -- This is not dense matrix to take into account only values

Sparse MAtrix is not a regular matrix with n rows and n columsn where we have values for all

Sparse MAtrix can be converted to regular matrix if required

fit_transform of countvectorizer is automatically giving a sparse matrix
97.png


TFIDF means Term Frequency Times Inverse Document Frequency tf-idf
MEans each token has its own importance and the more frequency it has , it is less important

TfidfTransformer

TfidfVectorizer

Tokenizing, counting are done by Count Vectorizer
Normalizing is done by TF-IDF

TfidfVectorizer -- can do it all together.




--End Jun 19

-- Start 18Jun
Support Vector Machines -- SVC -- sklearn 
Good for linear variables

Wisdom of the Crowd -- :) 
Views of multiple people as compared to the one intellectual -- e.g. KBC Polling

The applicability is 
Try for different results 

Instead of using the results of one model -- Take the results of the 3 models -- 83.png
Take the voting of 

Ensemble Data , classifier, predictor , model -- all mean the same
votingclassifier.ipynb
--voting classiifier comes from ensemble module

Creating Voting Classifier

Soft voting is possible only when we get probability
Hard voting is more binary i.e. 'Y', 'N'

Bagging, Boosting

Bagging - 84.png -- if there are 3 predictors , different sets of data are provided t

Voting classifiers uses diversified models and each model is fed with the same set of data
Bagging technique uses differnt bags(subsets -- remember the subsets can also mean different set of features and not just data) of data but fed through the same model
Bootstrap aggregation or BAgging

Random Forests is the best example for bagging

RandomForestClassifier

Decision Trees do not give soft voting because there is no calc of probability

voting classifier gives it 

Baggimg classifier gives you the option to select base estimate

Boosting is trying to create models in sequence

GradientBoosting
AddupBoosting
ensemble_models.ipynb

n number of classifiers or steps -- results are refactored based on the weigths of results at each step


--End 18Jun



-- Start 17JUN

DECISION TREES
decision tree does not require scaling
machine_learning material

if the number of branches in the decision tree are large, then it will go into overfitting i.e. trying to fit in the train data 

Which feature is to start with as the top most node?

this is to be based on impurity-index a.k.a gini-index, entropy will help in deciding that. gini-index is better but entropy is better balanced

from sklearn.tree import export_text

KNN - K nearest neighbour - K is nothingy but the count. Euclidean distance

Requires more memory 

from sklearn.neighbors import KNeighborsClassifier
For this data has to be scaled similar to logistic regression.

-- Naive Bayes
--Stable to data changes
--Ability to work with large data set
-- Called as Naive because it treats all features as equally important and it is based on Bayes Theorem which is conditional probability
-- 77.png
-- 78.png -- how to make a choice of models picture -- 79.png

-- END 17 JUN

-- Start 15 Jun
loan_prdiction.ipynb has got good examples of grouping data using data frames.
Data transformation of applicant income can be done using binning technique
Do Titanic case study or some other.

If a particular variable such as credit history is important and there are null valies -- then it is best to drop the rows. 

After wrangling -- data is written to final_loan.csv

remember x is input -- y is target

confusion matrix -- 48.png

Actual     Pred         Classify
Positive positive      True Positive
Negative Positive      False Positive
Negative Negative     True Negative


-- End 15 Jun
-- Start 14Jun
Classification
load_prediction.ipynb

If there is a lot of skewing in the distribution of the data -- in the loan example -- the applicant income
Techniques to reduce the skewing include transforming using log() or binning techniques
Univariate Analysis
Bivariate Analysis
--End 14 Jun
-- Start 12 Jun --
Datascience Life Cycle discussed.

-- End 12 Jun

-- Start 11Jun
Feature Scaling 
Goal is to put all the features in the same scale. For e.g. iin the example GRE is 11 + and cgpa is 0.6). This is needs to be scaled
2 techniques

Normalization

Standardization -- Standard Scaler -- 19.png, 20 and 21, 22.png of comparitive values for before and after

Only x i.e. features are to be scaled. Not the target.

The techniques may not always improve

24.png -- Remember Train Data has to follow the steps of fit and transform but for test data , only transformation is sufficient. No need to fit again *** very imp

Even for prediction -- we hvae to transform 
i.e. chances = model.predict(ss.transform([[320,110,8.5],[310,115,9.2]]

--Pickling Models -- Serialization
--Unpicking or Deserialization
Remember the data input to a model should always be an array even it if it is a single row. It should not be a vector
test_admissions.ipynb
test_lr_model.py

26.png -- list of items required to run the model
28.png --how end user interacts with the web app
29.png includes API
-- End 11 Jun
-- Start -09Jun
If Target -- Y is a continous value(numerical) -- it is called Regression
If Target -- Y is a Categorical Value i.e. Spam or Ham in emails -- Diseased -- Y or N -- it is called Categorical 

-- Supervised Learning is the mechanism by which The model helps the system to learn about the predictions. The target data should be present to help the Supervised algorithms to predict.

Unsupervised is when we dont have target historic data

steps for univariate analysis --

features are also called dimensions
it is important to identify which features to keep and which features to drop because not all features are important
This is called reducing dimensionality. Reducing features
By doing this -- model complexity is reduced -- especially useful if u r processing images

-- End 09 Jun


-- Start 08 Jun -- 
RElational Plot

sns.scatterplot() 
you can go by style and hue also

--
distplot() is figure level
histplot is histogram
barplot -- categorical value on x and continous values are on y


rugplot draws sticks to show the distribution of data


Multiplot 
Pairplot -- very interesting -- 65.png
seaborn_demowhat is kde

u get distribution as well as comparision

regression plot -- draws a regression line

lmplot() -- figure level regression plot

residual plot -- very interesting and nice

Face@grid

MAtrix Plots -- To draw heat maps -- 
first get pivot table

correlation matrix -- reltion  between total bill and tip. 

-- End 08 Jun


-- Start 07Jun
Stacked charts
Stacked bar is about giving bottom argument -- 
ax = plt.subplot()
ax.bar(smoker_total.index, height=smoker_total

--
smoker_total.plot.bar(stacked=True) -- # easier way
smoker_total is a pivot table and hence a compatible object 
--Multi Bar
smoker_total.plot.bar()
matplotlibdemo.ipynb--

smoker_total.plot.line() -- integration between pandas and matplotlib -- pandas gives the dataframe
Multi sub plots

-- Horizontal Bar
-- line graph
Histogram
ax=tips.hist()
ax[0,0].set_title("Table_size")
-- the above draws histograms for all columns. That is because tips.hist() is returning an array of dataframes


-- Countries -- get data for the countries via the api
from then on tut is data frame
-- Top10 countries by population in Asia
--Using matplot lib -- we can draw a number of graphs

--- Better than matplotlib is SeaBorn
designed for statistical graphs built on top of matplot lib and integrated with pandas data structures
*supports categorical variables

Relational Plots
Categorical Plots
Dist Plots
Regression Plots
Matrix Plots

--Seaborn Demo
import seaborn as sns
--tips and flights data sets
sns.set(style='dark') -- sets the theme

Categorical Plots
boxplot()

ax=sns.boxplot(x='day','y='tip',data=tips) -- just needs data and seaborn does the rest. Much simpler compared to matplotlib , extra paremeter -- showfliers=False wil remove outliers

--hue is saying it is going to draw -- for each x -- it will not draw 1 graph but it will draw 2 graphs . For e.g. hue="sex" it will draw a bar for each unique valie

--countplot() 
for each dawy -- how many transactions are there -- count plot gives that

Even for count plot, hue can be given 

whitegrid is an option that you can be provided
style = 'whitegrid' instead of dark

Difference between mmatplot lib and seaborn is -- in matplotlib, you have to set the data for grouping where as in seaborn -- it will do it , if we just specify the data and the x, y columns

-- Display data according to countries -- Total Area -- Average area
Once the data is in dataframe, it can be used to draw different stules

Categorical Plots
--Catplot
--these are fugure level
sns.catplot(,,,,col='smoker','') -- 2 different graphs for smoker and non smoker
in matplot -- you will have to define subplots and then do it

if we give col= 'day' it gives 4 graphs -- 1 for each day

The default estimator is mean -- which implies it gives the average biill value for each day 
subplots_adjust(top=0.75) -- will leave 25% margins overall at the top

-- you can also set rows 

--boxplot
catplot is a figure level 

kind parameter -- can be only 1 -- so, you cant have bar and line together for example

--stripplot
It is a scatter plot with 1 categorical value
stripplots can also use hue

--swarmplot -- swarmplot does not overlap-- it tries to mark each transaction clearly without overlapping
dodge is important only when you are using hue -- it clearly demarcates the lunch and dinner plots for examople

-- fligjts data -- timeseries data -- try various graphs
For timeseries data -- line graph is best. Other examples shareprice
senesexdata -- line graph
in seaborn -- u do not explicitly need to specify the grouping. estimator = sum -- will give the count of passengers on a given day.

-- linegraph is a relational map because it gives relation between x and y

figure plots are multiple plots



-- End 07 Jun


-- Start 05 Jun
Matplot lib -- matplot lb is open source version eqqivalent to matlap
seaborn is a wrap on top of matplot lib and better.

matplotlib is a subset of seaborn

Anatomy of matplot lib

Figure is top most. A figure can have multiple AXES. Each Axes corresponds to a distribution. Plot, graph, axes all mean the same
1 Axes can have multiple Axis -- usually x and Y

--Assignment -- Try to explode the largest Wedge. Find out the largest wedge and pull it out.
-- matplotlibdemo -- piechart section -- 17
-- Every object is a collection of objects aain

ax = plt.pie(....)
t = plt.title (...)
print(ax) -- interesting output -- returns list of all objects -- example wedge objects. 

-- matplot lib -- scatter plots -- examples are there but seaborn is better. Much better for graph especially scatter plots
#plt.gcf().set_size_inches -- gcf is get current figure

Boxplot -- is of distribution for data

histogram -- distribution of data -- specify bins 
-- Range is kind of an iterator -- u dont get any thing until you take a value from otu

--Object API -- so far we have used module API. Plt is a module API. Object API gives more control
ax= plt.subplot()
print(type(ax))
ax.bar(dat_total) etc

Why do we need object level api

to create multiple graphs with the same data

fig.ax = plt.subplots(1,2) -- -- array of graphs

you can save the multi graphs as jpg and attach it as an attachment to email. -- 45.png -- look at a practical implementation of Python usage for lambda.

--47.png for Text -- zip(days,totals) --Printing text on graphs.

-- Countries json

Dynamo DB stores data in items -- each item is a json object.
json is forefront. xml was very popular and it was overtaken by json.  -- 48.png
-- countries = pd.read_json()
-- stacked grapjs

use data other than tips

-- End 05 Jun

-- Start 04 Jun
pandas -- page 35. Grouping an dother aggregate functions
-- most of the functions applicable to data frames are also applicable to series.
--df.fillna() -- fills null values

whole fillna is about imputation -- i.e. a technique used to filling the null values with an appropriate value like -- mean, median or 0
other techniques are forward fill and backward fill. Forward fill will take the previous non null value and put it. Backward fill will take the non null value and fills the previous null value with that.

-- Binning is the technique of dividing data into groups and helps in removing outliers for e.g.
-- Histogram is internally binning the data first and then it is drawing the plot
(85.0, 100.0] representation in maths indicates 85 -- parenteheses is included , ] square bracket --non inclusive i.e. 100 is not included

qcut -- is going dividde the data based on the percentile -- quantile based discretisation function

Categorical FEatures are to be Encoded -- 1 technique is to replace it with numbers.
But the problem with the number replacement is -- for e.g. if Male is encoded with 0 and F is 1. During calculations F will be treated as higher when compared to other.
In such cases, we create dummy columns
------Dummies
getdummies will retrieve all non-numberic columns   -- 42.png 
also called as 1 or/heart encoding
Equivalent to junk dimension probablu?

1 heart encoding will increase the number of columns.

-- Writing Data frame -- to json, to file 
orient is important for the json structure
each row should be saved as a json object
if we orient with records

Last thing in pandas is plotting -- thought it is not the focus


-- End 04 Jun

-- Start 03 Jun
data management , grouping , pivot tables etc
-- End 03 Jun

--Start 02Jun--
Pandas
at() 
iat() dont support slicing and hence faster than loc and iloc
stick to loc if u r dealing with rows and columns. It is easier and intutive. 
df.loc[:8,:'subject'] # Rows upto row-level 8, cols upto subject
iloc -- is based on indexes rather than labels

df.filter(regex = 'e$') # select columns where column name ends with Letter e
df.filter(regex='[0-9]+$') # select columns ending with a numeric
df.filter(egex='6',axis=0) -- filter is all about labels and not data. This filters rows with row index 6

vecotrization is a term used to indicate that an operation is applied across the whole of the array
df.nlargest(4,'marks') # keep=first -- indicates even if there are duplicate values , only first one will be fetched
apply() and applymap() -- 022.png, 23.png

df.apply(lambda s: s.notnull().sum().axis =1) -- apply operation on rows -- default is 0 which means columns

In applymap -- every alue of the data frame is passed. 
df.applymap(lambda v:str(v).upper()) # will convert every element into upper.

df.name.upper() gives a value because upper() cannot be applied on series
--solution is -- df.name.str.upper() -- each value in the name series is converted to string and then subsequently apply any string related operation. Very powerful
--Masking is the term used to use 2 boolean conditions together. e.g. where a > 10 and a <15
--> Attribute str
df.name is a series and it is a collection of values. If we have to take each value in the series should be converted to uppercase. Perform an operation on each value. If we dont want to use apply.

-- look at screen short 28 for example or regular expression grouping df.name.str.extract('([A-Z+])([A-Z]+)') -- GIVES 2 columns corresponding to the 2 regular expression groups

-- look at tips_df.ipynb -- experiment on tipsdf

import pandas as pd
import seaborn as sns

tips = sns.load_dataset('tips')

github.com -- seaborn


-- Assignment
1) display rows where day is starting with S
2) display rows where the percentage of tip amount to total bill is greater than 10%
3) display transactions for male smokers
4)display where size  is 3, 4 or 5 
5)display integer portion of total bill along with day and time
6)display unique days -- 
7) top 10 bill amounts
8) display rows where row index contains digit 9
9) display rows where day is sat or sun, non smoker , male customer with table size > 2
10) display [odd rows and columns] that contain letter 'O' (letter O)


-- End 02 Jun



-- Start 31 May
Image processing
import matplotlib.pyplot as plt
import matplotlib.image as image
from scipy import misc

pandas -- loc -- exclusive
iloc inlusive

-- understand row positions, row labels
--- Exercises related to pandas -- assignments
-- Display months where value is less than previous months -- series_demo.ipynbt -- 59

-- End 31 May

-- Start 29 May
Be careful with horizontal and vertical splits. It might be confusing the way we think. Horizontal split happens if you imagine a vertical line in between the elements.
look at oage 30 examples of the course material.

Linear Algebra
np.linalgeb

-- Remember reshape does not change the shape in place. It gives a view
--- No of columns and the number of rows in the second should be equal if multiplication has to be successful

Dot product is different from matrix product -- check examples

 y = a + b1x1 + b2x2
 
 x 1 1000 2
   1 1500 3
   1 1800 3

weights arrau
a   2000000
b1    5000
b2   5000000

x @ w -- coefficinets /weights -- look at coef example  x @coef  -- 68.png
above is linear regression using linear algebra

----Broadcasting 
is the concept -- when 2 arrays are not of the same size. To bring them to the same size, numpy uses broadcasting 

simple example is scalar broadcasting. 
for e,g 
[1 2 3] + 10
[1 2 3] + [10 10 10]
[11 12 13]
Same logic for 2 dimensional as well -- slide 70 -- broadcast 2 example


-- End 29 May

-- Start 28 May
axis = 0 will retrieve data column wise
axis = 1 will retrieve data row wise i.e. student wise
e.g.
marks.sum(axis=1) 
marks[::2].sum() -- alternate rows sum

argmax() will give the index/position of the element that has the max value
argmin() will give the index/position fo the element that has min value
argsort() wil give indexes of the sorting order. Not the actual values

sa = a[a.argsort()]

screen shot 56
a and b are 2 different objects which have different ids but they point to the same memory. When you slice -- u get a view.

if we wamt to get a copy and not a view
a2 = a.copy() -- will give a copy of that into a2. Each one will have its own memory location.

arrays extracted based on a boolean index and array of indexes is a copy and not a view. Unlike slicing.


universal functions -- page20 -- numpy
-- stacking and splitting -
Horizontal stacking  - 

2 x 2 -- 2 x3 -- as long as the number of rows is same then horizontal stacking is possible
vertical stacking as long as the columns are same
-- End 28 May

-- Start 27 MAy
Boolean Indexing
Boolean index and the array index should have the same number of elements
Very powerful concept

When ever boolean indexing is applied on an array -- the resultant is a vector not a corresponding array of same size. Reason is that the number of elements 
In each row , the number of elements being selected and returned couuld vary depending on the condition
as we dont have the same number of elements in each row -- hence returns a single dimension array -- vector

numpy_demo.ipynb -- class room note book

Masking 

(a > 20) & (a < 40) -- 2 boolean indexes  -- remember to use & which is the mask. In python  # a [(a > 2 and a < 8 )] -- #Anding in python does not work here

a[prime(a)] -- prime(a) is a function which is generating a list of booleans. 

Boolean indexing is a feature of numpy and panda libraries. It is not available in regular programming languages

a.mean() is a method because any thing you call with an object is a method -- a method is always called with an object

np.add(x,y) -- function. Functions are universal and are called with the package  -- important differentiation

a1 = np.arange(5)
a2 = np,arange(5)
a1 == a2 # This will compare each element of the array a1 with the corresponding element in the array a2. And returns an array of booleans
a1.__eq__(a2) -- internal represemntation of a1 == a2. 

all() or any() methods -- if all elements are True , then all wil return True. else False

(a1==a2).all() -- Every element in the first array
--Methods of ndarray
a = np.random.randint(1,100,(6,6))

-- End 27 May 


-- start 21 May
formatting options using mark down
bold , italic, monospace. Monospace is a where all characters maintain an equal space. Monospace can be defined with a back quote

Refere notes for red, yello, green, blue -- alert, warning etc boxes iusing css classes.

Image can also be put into a note book

myurl = "https://www....."
from IPython.display import Image
Image(url = myurl)
jupyter notebooks
-- enf 21 May
--- shell commands
Shell commands start with exclamation
!cd,  type etc -- demo.ipynb
--Magic Fuctions
Magic functions are of 2 types.
Line magic and cell magic
Line magic starts witjh % and takes the rest of the line as command
cell magic starts with %% and takes the rest of the cell as command
e.g. %pwd
%env
env = %env -- will give a dictionary of env variables assigned to env
%who will return al variables created so far
-- cell magic

nbviewer.jupyter.org

you can give the github url into that and the note book viewer will output the data.







-- end 21May



sTART - 17 maY
HAmming distance -- it is a % disagreement between 2 sequence values. For e.g. [1,2,3 [1,2,4] -- 33.33%
distance = hamming(user1_ratings,user2_ratings)

Other distances are Eucladian

Example provided in class is movies_recommendations. Other use cases are books

eND - 17 mAY

/opt/anaconda3/bin/jupyter-lab
https://github.com/srikanthpragada/DS_05_APR_2021

--09-Apr
Read about sample collection techniques
scipy is scientific python

Categorical -- example gender , branch - discrete values

Contionous -- E.g. Marks, Salaries

Mode is important when it is categorical

-- End 09April class

-- Start 10April

Exercise

Create 50 random numbers in the range 1 to 1000 and display lower outliers and upper outliers
-- End 10th April


Start 12-April

For Jupyter notebook viewer
Data Frame and Series
Column is like a series
df.salary.mean()  means salary is being used like a list
df['salary'].median()

In box plot, you can display outliers where as histogram, outliers arent shown
s.plot.box(showfilers=false) -- to turn of out liers from box plot
https://nbviewer.jupyter.org/github/srikanthpragada/DS_05_APR_2021/blob/master/stats/demo.ipynb

--Correlation -- to understand if there is an influence of one variable on another. For example, experience and salary
p value -- randomness in relationship -- the much lesser, the much better

coefficient -- high coefficient is a strong relationship
End - 12th April


--Start - 14April
Linear Regression
Slope is very important. 
Price = intercept + slope * size 
y=a+bX
y indicates dependent variable. x is independent variable.
Intercept is Y value when x = 0 
Example -- cab charges -- initial intercept(base price) + km * price per km

If there are more than 1 variable -- for e.g. Size, bedrooms , age

5000 per sq/ft
-100,000 for age
200,000 for additional bedroom

There is only intercept but multiple slopes
y = a + (b1 * x1) + (b2 * x2) + (b3 * x3)
Example
price = 0 + (100 * 5000) + (5  * -100000) + (3 * 200,000)

Error -- distance between the prediction and actual is called error or Residual
The goal is to adjust the slope in such a way that it is close to reality

Mean Squared Error -- MSE -- to remove sign and

Sum (MSE)/n

-- End of 14 April

-- Start of 15 April
a = np.eye(5,5) -- eye -- identity matrix -- 1 is filled in diagonal

np.linspace -- linearspace
-- End of 15 April

-- Start 16th April
Slicing is a concept in python. Spend some time understandig slicing in python

Assignment -- Mini
Take an array of 5 x 5. Fill it with random numbers and display numbers that are greater than the average of the whole array.

Python for data analysis Wes McKinney, covers Panda's and Numpy
Python Data Science Handbook Jake Vanderplas
-- End 16th April

-- Start 17-April -- Methods of Array -- Methods of ndarray
a.mean() -- is called ndarray methid
print(np.add(a,b)) -- Universal function -- it is not a method -- because u r calling it with numpy library and not method of an object. 
Method is always a function of an object and hence object.method is how it is usually called.

What if u want to sort the whole 2 dimension array? -- Assignment

numpy.org -- for full numpy documentation

-- End 17th April

-- Start 19th April - Monday --- Copy vs View

-- Start 20th April
Coefficient -- coeff.T -- transpose

X.dot(coeff). Addint 1 as one of the features and coefficient at the beginning is imp

Broadcasting -- Smaller array is broadcast to larger array to perform operations


Assignment
How many pixels have more than 100 for red? 
If red is more than 100, reduce it to 100.

-- End 20th April

-- Start -- 21st April 
Pandas -- Series and Datframes
Series is a function
Index in Series is also known as rowlabel
Index can be nonnumeric and non-unique

Elements in a series can be accessed by position also. For e,g - s[0]
Error will be returned in case the custom index is also numeric and the position given is not defined as part of the custom numeric index. To overcome use the properties loc and iloc
loc is purely based on custom index. iloc is based on position index		


In dictionary -- key is unique
In series -- index can be duplicated

Display values in a series where there is a decrease from a previous value.

ndarray means numpy array
-- End 21st April
-- Start 22nd April
Important powerful methods
--apply
marks.apply(get_grade)
equivalent function in python is map 
map(get_grade, arks) but you need to write a for loop for printing
for v in map(get_grade,marks):
print(v) 

Slicing is important topic

You cant use loc for sorting and displaying top 2.
Because the index keeps changing. loc is position based on position and hence will display in correct values
for e.g. narks.sort_values(ascending=False).loc[:3] will all values until position 3 which is all values
For index based retrieval -- use loc
For position based retrieval -- use iloc

Fill nulls with values is called Imputing the values

When ever we use [] or loc[] it uses index
iloc[] will use position
pandas.pydata.org
when u get info from data frame -- if data type is object -- it means string
df.info()
-- End 22nd April

-- Start 23rd April
Retrieiving data from data frames

-- Normal Slicing, LOC and ILOC
at, iat, loc, icloc
at and iat do not support slicing
at -- only for specific row and column and not for slicing

e.g. print(df.at[0,'marks']) -- # 1st row -- marks column value
print(df.iat[0,2]) -- #1st row, 2nd column value

df.head() -- first 5
df.tail() -- last 5
df.sample(5) -- 5 random rows
df.subject.isin['Python','Java'] -- compare with a alist of values
Exercise -- Subject not java , marks less than 50

df.where(df.marks > 80)

df.filter -- filter is for labels. Not for data but for row or column labels
df.nlargest(4,'Marks') -- pick rows with first 4 highest marks

df['name'].drop_duplicates gives an array
df['name'].unique gives series
seaborn library
-- End 23-April

-- Start 24 April
df.apply  can be used to find out not null values fy axis = 1ws. If rows are required, s[pecify axis = 1
df.applymap() -- the function is applied to all rows and columns
df.name.upper() -- retirns error - upper cannot be used on series
instead use df.name.str.upper()
KM58 VDK

tRUE MEANS FALSE, PASS MEANS FAIL
ASSIGNMENT -- Print Pass or Fail not True or False

df.drop([11,13]) will remove rows but for a new copy
df.drop([12,13],inplace=True) will remove rows from the original
axis=0 is rows. depends on function.

Assignment
Create a new csv file books, movies or something else. 10rows
-- End 24 April

-- Start 26 April
df.remane(columns = {'rollno': 'admno'}
Concatenation, Joining
pd.concate((df1,df2), ignore_index=True)

If ignore_index=True is not set, then the indexes of the original data frames are takenn

joining in data frames is by default outer index.
The inner join is based on index

pd.concat
pd.merge -- merge , merges data based on common column
df1.merge(df2,how='oter') for outer join

Grouping
df.groupby('subject') -- # data frame group by object
grouping is for summaries 

subgroup = df.groupby('subject')

use tips data to see grouping happenin on multiple numeric columns

tips.groupby('sex')[[total_bill','tip']].mean()
grouping on multiple columns is possible
pandas is not just for data science. It is also primarily
for data analysis
-- End 26 April
-- Start 27 April

Data Plotting, Wrangling and Pivot Table

summary = tips.groupby(['day','time']).index
summary = tips.groupby(['day','time']),as_index=False).sum()

get_dummies()

pd.get_dummies(tips[['smoker','day']])
also called as 1 heart/or encoding
tips.pivot_table(values = 'total_bill', index = 'day', columns='sex',aggfunc=['sum','mean'])

Binning
tips['bill_bin']=pd.cut(tips.total_bill,5)
tips[['total_bill','bill_bin']]
Binning is useful when we have outliers
Using Log is another method of removing the effect of outliers so that ML algorithms can work correctly

pd.qcut(tips['total_bill'],[0,0.25,0.75,1], labels['LQ', 'UQ','IQR'])

pivot_table.plot.bar()

latest payslip
latest bank
credit report

techmyfile
-- End 27th April

-- Start 28th April
Matplotlib is a python 2d plotting library. There is an extension to draw 3d also.

-- End 28th April 

-- Start 29th April

-- End 29th April

--Start 30 April
Swarm Plot
Categorical Plots

Distance between the point and the line is called residual
If residual is on 0, then there is a difference between predict
and actual
Heat Map 
Correlation Matrix

seaborn is better than matplotlib

--End 30 April

--04-May- Start
pickling
deployment
zhango framework
Data Science Life Cycle
Feature Engg is used in Exploratory Data Analyis
Not all features need to be used for prediction.
In this phase , you can drop certain features after analysis
--04-MAy End

--05 May Start
Classification case study -- loan_prediction.ipynb
train = pd.read("loan.csv") -- train is the data frame
train.shape() will give you rows and columns 
outout -- 623,13
train.sample(5) will give a sample data of 5 rows
train.info() -- gives info about the data -- not null etc
train.describe() more info
train[['ApplicantIncome','CoapplicantIncome']] subset of values
train
-- 05 May End

--08-May-Start
different models
--08-May-End

-- 10-May Start
Train , Validation and Test data sets in the range of 60%, 20%, 20%

Cross validation will divide the data into K-Fold . If k =4 it will split into 4 sets. For each data set, a model is created.
The advantage is -- every observation is used in training and every observation is used in testing

model = 
scores = cross_val_score(model, train, y, cv = 5)

some of the algorithms use cross validation

E.g. LogisticRegressionCV
train_test_split?

confusion matrix gives an array of true positives and true negatives
Amber
--> Ensemble Methods
Most of the competitions were won by individuals who used Ensemble algorithms

load_iris from sklearn
iris data is toy data
from sklearn.datasets import load_iris

d= load_iris()
print(d.DESCR)

X, y = load_iris(return_X_y=True)
votingclassifier.ipynb
page 68 of course material
Bagging means --  Bootstrap Aggregation
We build the same model but it will take different set of training observations
Again Model voting is done
ensemble_models.ipynb

Genie Index
--10 May End

--Start 11-May
Technique Boosting
XGBoost -- in competitions if u want to win
Gradient boosting classifier which internally uses different decision trees
Stacking
Voting classifier is different

svc and knn require scaling

test_model.ipynb -- plots and curves -- advanced classification concepts

u have to use the same tfidf

---Sentiment Analysis

Bag of words
2 types of matrices
sparse matrix, dense matrix
hotrl_reviews.ipynb
--End 11-May

-- Start 12-May
Case Study 2
car/prepare_data.ipynb
gradient decent ,  regularisation, pipeline, how to create one, Efficient ways to check multple hyper parameters. Grid search
use case

df.isin([?]).sum() -- gives counts of values in each column which have ?
Finding the price of a car
price_finder_models.ipynb

-- End 12-May
--> 14- May Start
-- Parameters are values that the model generates 
-- Hyper parameters are for e.g. k in k nearest neighbours which are used to 
--Grid search is to find best hyper parameters
--Gradient decent is to find best parameters
is sklearn -- random_state = (1 or 12 0r 100 or any value) if we have to keep random state consitent
blob_demo
matplot lib is very important for scatter plots

blob_demo.ipynb for un supevised learning
for k means - always scale data
make_moons.ipynb
DBSCAN - density based clustering -- another technique
In dbscan -- u dont mention how many clusters are required. The algorithm figures it out
-- 14 May End

--Start 15-May
Page  101
DBSCAN
eps - epsilon or radius
customer_seg.ipynb
what should be the ideal epsilon -- radius? reducing epsilon will increase clusters and increasing will reduce them

Hierarchial clustering - Aglomerative -- suitable for small clusters
, Divisive -- suitable for large clusters
cluster map
seaborn.clustermap
dendrogram using scipy
ward is a function to calc distance

kmeans -- centroid -- dont place centroids randomly

unsupervised - Association - REcommender systems
REcommender system is an example of unsupervised machine learning using association
candidate generation
-- reduces billions down to hundreds of thousands
Scoring

--Re-ranking
Recommender - system -- content based and collaborative filtering

scoring
Assignment -- Display all border points in a different colour

implicit

Recommender System - Collaborative filtering
Predict user ratings for products based on a user's similarity with other users
It may be user based or item based
item based - neighbours

-- End - 15 May
-- Start - 16May
Recommending movies in collaborative filtering
movies_cf_final.ipynb
-- go through sides and understand clustering
