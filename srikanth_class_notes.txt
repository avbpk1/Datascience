-- Start 15 Jun
loan_prdiction.ipynb has got good examples of grouping data using data frames.
Data transformation of applicant income can be done using binning technique
Do Titanic case study or some other.

If a particular variable such as credit history is important and there are null valies -- then it is best to drop the rows. 

After wrangling -- data is written to final_loan.csv

remember x is input -- y is target

confusion matrix -- 48.png

Actual     Pred         Classify
Positive positive      True Positive
Negative Positive      False Positive
Negative Negative     True Negative


-- End 15 Jun
-- Start 14Jun
Classification
load_prediction.ipynb

If there is a lot of skewing in the distribution of the data -- in the loan example -- the applicant income
Techniques to reduce the skewing include transforming using log() or binning techniques
Univariate Analysis
Bivariate Analysis
--End 14 Jun
-- Start 12 Jun --
Datascience Life Cycle discussed.

-- End 12 Jun

-- Start 11Jun
Feature Scaling 
Goal is to put all the features in the same scale. For e.g. iin the example GRE is 11 + and cgpa is 0.6). This is needs to be scaled
2 techniques

Normalization

Standardization -- Standard Scaler -- 19.png, 20 and 21, 22.png of comparitive values for before and after

Only x i.e. features are to be scaled. Not the target.

The techniques may not always improve

24.png -- Remember Train Data has to follow the steps of fit and transform but for test data , only transformation is sufficient. No need to fit again *** very imp

Even for prediction -- we hvae to transform 
i.e. chances = model.predict(ss.transform([[320,110,8.5],[310,115,9.2]]

--Pickling Models -- Serialization
--Unpicking or Deserialization
Remember the data input to a model should always be an array even it if it is a single row. It should not be a vector
test_admissions.ipynb
test_lr_model.py

26.png -- list of items required to run the model
28.png --how end user interacts with the web app
29.png includes API
-- End 11 Jun
-- Start -09Jun
If Target -- Y is a continous value(numerical) -- it is called Regression
If Target -- Y is a Categorical Value i.e. Spam or Ham in emails -- Diseased -- Y or N -- it is called Categorical 

-- Supervised Learning is the mechanism by which The model helps the system to learn about the predictions. The target data should be present to help the Supervised algorithms to predict.

Unsupervised is when we dont have target historic data

steps for univariate analysis --

features are also called dimensions
it is important to identify which features to keep and which features to drop because not all features are important
This is called reducing dimensionality. Reducing features
By doing this -- model complexity is reduced -- especially useful if u r processing images

-- End 09 Jun


-- Start 08 Jun -- 
RElational Plot

sns.scatterplot() 
you can go by style and hue also

--
distplot() is figure level
histplot is histogram
barplot -- categorical value on x and continous values are on y


rugplot draws sticks to show the distribution of data


Multiplot 
Pairplot -- very interesting -- 65.png
seaborn_demowhat is kde

u get distribution as well as comparision

regression plot -- draws a regression line

lmplot() -- figure level regression plot

residual plot -- very interesting and nice

Face@grid

MAtrix Plots -- To draw heat maps -- 
first get pivot table

correlation matrix -- reltion  between total bill and tip. 

-- End 08 Jun


-- Start 07Jun
Stacked charts
Stacked bar is about giving bottom argument -- 
ax = plt.subplot()
ax.bar(smoker_total.index, height=smoker_total

--
smoker_total.plot.bar(stacked=True) -- # easier way
smoker_total is a pivot table and hence a compatible object 
--Multi Bar
smoker_total.plot.bar()
matplotlibdemo.ipynb--

smoker_total.plot.line() -- integration between pandas and matplotlib -- pandas gives the dataframe
Multi sub plots

-- Horizontal Bar
-- line graph
Histogram
ax=tips.hist()
ax[0,0].set_title("Table_size")
-- the above draws histograms for all columns. That is because tips.hist() is returning an array of dataframes


-- Countries -- get data for the countries via the api
from then on tut is data frame
-- Top10 countries by population in Asia
--Using matplot lib -- we can draw a number of graphs

--- Better than matplotlib is SeaBorn
designed for statistical graphs built on top of matplot lib and integrated with pandas data structures
*supports categorical variables

Relational Plots
Categorical Plots
Dist Plots
Regression Plots
Matrix Plots

--Seaborn Demo
import seaborn as sns
--tips and flights data sets
sns.set(style='dark') -- sets the theme

Categorical Plots
boxplot()

ax=sns.boxplot(x='day','y='tip',data=tips) -- just needs data and seaborn does the rest. Much simpler compared to matplotlib , extra paremeter -- showfliers=False wil remove outliers

--hue is saying it is going to draw -- for each x -- it will not draw 1 graph but it will draw 2 graphs . For e.g. hue="sex" it will draw a bar for each unique valie

--countplot() 
for each dawy -- how many transactions are there -- count plot gives that

Even for count plot, hue can be given 

whitegrid is an option that you can be provided
style = 'whitegrid' instead of dark

Difference between mmatplot lib and seaborn is -- in matplotlib, you have to set the data for grouping where as in seaborn -- it will do it , if we just specify the data and the x, y columns

-- Display data according to countries -- Total Area -- Average area
Once the data is in dataframe, it can be used to draw different stules

Categorical Plots
--Catplot
--these are fugure level
sns.catplot(,,,,col='smoker','') -- 2 different graphs for smoker and non smoker
in matplot -- you will have to define subplots and then do it

if we give col= 'day' it gives 4 graphs -- 1 for each day

The default estimator is mean -- which implies it gives the average biill value for each day 
subplots_adjust(top=0.75) -- will leave 25% margins overall at the top

-- you can also set rows 

--boxplot
catplot is a figure level 

kind parameter -- can be only 1 -- so, you cant have bar and line together for example

--stripplot
It is a scatter plot with 1 categorical value
stripplots can also use hue

--swarmplot -- swarmplot does not overlap-- it tries to mark each transaction clearly without overlapping
dodge is important only when you are using hue -- it clearly demarcates the lunch and dinner plots for examople

-- fligjts data -- timeseries data -- try various graphs
For timeseries data -- line graph is best. Other examples shareprice
senesexdata -- line graph
in seaborn -- u do not explicitly need to specify the grouping. estimator = sum -- will give the count of passengers on a given day.

-- linegraph is a relational map because it gives relation between x and y

figure plots are multiple plots



-- End 07 Jun


-- Start 05 Jun
Matplot lib -- matplot lb is open source version eqqivalent to matlap
seaborn is a wrap on top of matplot lib and better.

matplotlib is a subset of seaborn

Anatomy of matplot lib

Figure is top most. A figure can have multiple AXES. Each Axes corresponds to a distribution. Plot, graph, axes all mean the same
1 Axes can have multiple Axis -- usually x and Y

--Assignment -- Try to explode the largest Wedge. Find out the largest wedge and pull it out.
-- matplotlibdemo -- piechart section -- 17
-- Every object is a collection of objects aain

ax = plt.pie(....)
t = plt.title (...)
print(ax) -- interesting output -- returns list of all objects -- example wedge objects. 

-- matplot lib -- scatter plots -- examples are there but seaborn is better. Much better for graph especially scatter plots
#plt.gcf().set_size_inches -- gcf is get current figure

Boxplot -- is of distribution for data

histogram -- distribution of data -- specify bins 
-- Range is kind of an iterator -- u dont get any thing until you take a value from otu

--Object API -- so far we have used module API. Plt is a module API. Object API gives more control
ax= plt.subplot()
print(type(ax))
ax.bar(dat_total) etc

Why do we need object level api

to create multiple graphs with the same data

fig.ax = plt.subplots(1,2) -- -- array of graphs

you can save the multi graphs as jpg and attach it as an attachment to email. -- 45.png -- look at a practical implementation of Python usage for lambda.

--47.png for Text -- zip(days,totals) --Printing text on graphs.

-- Countries json

Dynamo DB stores data in items -- each item is a json object.
json is forefront. xml was very popular and it was overtaken by json.  -- 48.png
-- countries = pd.read_json()
-- stacked grapjs

use data other than tips

-- End 05 Jun

-- Start 04 Jun
pandas -- page 35. Grouping an dother aggregate functions
-- most of the functions applicable to data frames are also applicable to series.
--df.fillna() -- fills null values

whole fillna is about imputation -- i.e. a technique used to filling the null values with an appropriate value like -- mean, median or 0
other techniques are forward fill and backward fill. Forward fill will take the previous non null value and put it. Backward fill will take the non null value and fills the previous null value with that.

-- Binning is the technique of dividing data into groups and helps in removing outliers for e.g.
-- Histogram is internally binning the data first and then it is drawing the plot
(85.0, 100.0] representation in maths indicates 85 -- parenteheses is included , ] square bracket --non inclusive i.e. 100 is not included

qcut -- is going dividde the data based on the percentile -- quantile based discretisation function

Categorical FEatures are to be Encoded -- 1 technique is to replace it with numbers.
But the problem with the number replacement is -- for e.g. if Male is encoded with 0 and F is 1. During calculations F will be treated as higher when compared to other.
In such cases, we create dummy columns
------Dummies
getdummies will retrieve all non-numberic columns   -- 42.png 
also called as 1 or/heart encoding
Equivalent to junk dimension probablu?

1 heart encoding will increase the number of columns.

-- Writing Data frame -- to json, to file 
orient is important for the json structure
each row should be saved as a json object
if we orient with records

Last thing in pandas is plotting -- thought it is not the focus


-- End 04 Jun

-- Start 03 Jun
data management , grouping , pivot tables etc
-- End 03 Jun

--Start 02Jun--
Pandas
at() 
iat() dont support slicing and hence faster than loc and iloc
stick to loc if u r dealing with rows and columns. It is easier and intutive. 
df.loc[:8,:'subject'] # Rows upto row-level 8, cols upto subject
iloc -- is based on indexes rather than labels

df.filter(regex = 'e$') # select columns where column name ends with Letter e
df.filter(regex='[0-9]+$') # select columns ending with a numeric
df.filter(egex='6',axis=0) -- filter is all about labels and not data. This filters rows with row index 6

vecotrization is a term used to indicate that an operation is applied across the whole of the array
df.nlargest(4,'marks') # keep=first -- indicates even if there are duplicate values , only first one will be fetched
apply() and applymap() -- 022.png, 23.png

df.apply(lambda s: s.notnull().sum().axis =1) -- apply operation on rows -- default is 0 which means columns

In applymap -- every alue of the data frame is passed. 
df.applymap(lambda v:str(v).upper()) # will convert every element into upper.

df.name.upper() gives a value because upper() cannot be applied on series
--solution is -- df.name.str.upper() -- each value in the name series is converted to string and then subsequently apply any string related operation. Very powerful
--Masking is the term used to use 2 boolean conditions together. e.g. where a > 10 and a <15
--> Attribute str
df.name is a series and it is a collection of values. If we have to take each value in the series should be converted to uppercase. Perform an operation on each value. If we dont want to use apply.

-- look at screen short 28 for example or regular expression grouping df.name.str.extract('([A-Z+])([A-Z]+)') -- GIVES 2 columns corresponding to the 2 regular expression groups

-- look at tips_df.ipynb -- experiment on tipsdf

import pandas as pd
import seaborn as sns

tips = sns.load_dataset('tips')

github.com -- seaborn


-- Assignment
1) display rows where day is starting with S
2) display rows where the percentage of tip amount to total bill is greater than 10%
3) display transactions for male smokers
4)display where size  is 3, 4 or 5 
5)display integer portion of total bill along with day and time
6)display unique days -- 
7) top 10 bill amounts
8) display rows where row index contains digit 9
9) display rows where day is sat or sun, non smoker , male customer with table size > 2
10) display [odd rows and columns] that contain letter 'O' (letter O)


-- End 02 Jun



-- Start 31 May
Image processing
import matplotlib.pyplot as plt
import matplotlib.image as image
from scipy import misc

pandas -- loc -- exclusive
iloc inlusive

-- understand row positions, row labels
--- Exercises related to pandas -- assignments
-- Display months where value is less than previous months -- series_demo.ipynbt -- 59

-- End 31 May

-- Start 29 May
Be careful with horizontal and vertical splits. It might be confusing the way we think. Horizontal split happens if you imagine a vertical line in between the elements.
look at oage 30 examples of the course material.

Linear Algebra
np.linalgeb

-- Remember reshape does not change the shape in place. It gives a view
--- No of columns and the number of rows in the second should be equal if multiplication has to be successful

Dot product is different from matrix product -- check examples

 y = a + b1x1 + b2x2
 
 x 1 1000 2
   1 1500 3
   1 1800 3

weights arrau
a   2000000
b1    5000
b2   5000000

x @ w -- coefficinets /weights -- look at coef example  x @coef  -- 68.png
above is linear regression using linear algebra

----Broadcasting 
is the concept -- when 2 arrays are not of the same size. To bring them to the same size, numpy uses broadcasting 

simple example is scalar broadcasting. 
for e,g 
[1 2 3] + 10
[1 2 3] + [10 10 10]
[11 12 13]
Same logic for 2 dimensional as well -- slide 70 -- broadcast 2 example


-- End 29 May

-- Start 28 May
axis = 0 will retrieve data column wise
axis = 1 will retrieve data row wise i.e. student wise
e.g.
marks.sum(axis=1) 
marks[::2].sum() -- alternate rows sum

argmax() will give the index/position of the element that has the max value
argmin() will give the index/position fo the element that has min value
argsort() wil give indexes of the sorting order. Not the actual values

sa = a[a.argsort()]

screen shot 56
a and b are 2 different objects which have different ids but they point to the same memory. When you slice -- u get a view.

if we wamt to get a copy and not a view
a2 = a.copy() -- will give a copy of that into a2. Each one will have its own memory location.

arrays extracted based on a boolean index and array of indexes is a copy and not a view. Unlike slicing.


universal functions -- page20 -- numpy
-- stacking and splitting -
Horizontal stacking  - 

2 x 2 -- 2 x3 -- as long as the number of rows is same then horizontal stacking is possible
vertical stacking as long as the columns are same
-- End 28 May

-- Start 27 MAy
Boolean Indexing
Boolean index and the array index should have the same number of elements
Very powerful concept

When ever boolean indexing is applied on an array -- the resultant is a vector not a corresponding array of same size. Reason is that the number of elements 
In each row , the number of elements being selected and returned couuld vary depending on the condition
as we dont have the same number of elements in each row -- hence returns a single dimension array -- vector

numpy_demo.ipynb -- class room note book

Masking 

(a > 20) & (a < 40) -- 2 boolean indexes  -- remember to use & which is the mask. In python  # a [(a > 2 and a < 8 )] -- #Anding in python does not work here

a[prime(a)] -- prime(a) is a function which is generating a list of booleans. 

Boolean indexing is a feature of numpy and panda libraries. It is not available in regular programming languages

a.mean() is a method because any thing you call with an object is a method -- a method is always called with an object

np.add(x,y) -- function. Functions are universal and are called with the package  -- important differentiation

a1 = np.arange(5)
a2 = np,arange(5)
a1 == a2 # This will compare each element of the array a1 with the corresponding element in the array a2. And returns an array of booleans
a1.__eq__(a2) -- internal represemntation of a1 == a2. 

all() or any() methods -- if all elements are True , then all wil return True. else False

(a1==a2).all() -- Every element in the first array
--Methods of ndarray
a = np.random.randint(1,100,(6,6))

-- End 27 May 


-- start 21 May
formatting options using mark down
bold , italic, monospace. Monospace is a where all characters maintain an equal space. Monospace can be defined with a back quote

Refere notes for red, yello, green, blue -- alert, warning etc boxes iusing css classes.

Image can also be put into a note book

myurl = "https://www....."
from IPython.display import Image
Image(url = myurl)
jupyter notebooks
-- enf 21 May
--- shell commands
Shell commands start with exclamation
!cd,  type etc -- demo.ipynb
--Magic Fuctions
Magic functions are of 2 types.
Line magic and cell magic
Line magic starts witjh % and takes the rest of the line as command
cell magic starts with %% and takes the rest of the cell as command
e.g. %pwd
%env
env = %env -- will give a dictionary of env variables assigned to env
%who will return al variables created so far
-- cell magic

nbviewer.jupyter.org

you can give the github url into that and the note book viewer will output the data.







-- end 21May



sTART - 17 maY
HAmming distance -- it is a % disagreement between 2 sequence values. For e.g. [1,2,3 [1,2,4] -- 33.33%
distance = hamming(user1_ratings,user2_ratings)

Other distances are Eucladian

Example provided in class is movies_recommendations. Other use cases are books

eND - 17 mAY

/opt/anaconda3/bin/jupyter-lab
https://github.com/srikanthpragada/DS_05_APR_2021

--09-Apr
Read about sample collection techniques
scipy is scientific python

Categorical -- example gender , branch - discrete values

Contionous -- E.g. Marks, Salaries

Mode is important when it is categorical

-- End 09April class

-- Start 10April

Exercise

Create 50 random numbers in the range 1 to 1000 and display lower outliers and upper outliers
-- End 10th April


Start 12-April

For Jupyter notebook viewer
Data Frame and Series
Column is like a series
df.salary.mean()  means salary is being used like a list
df['salary'].median()

In box plot, you can display outliers where as histogram, outliers arent shown
s.plot.box(showfilers=false) -- to turn of out liers from box plot
https://nbviewer.jupyter.org/github/srikanthpragada/DS_05_APR_2021/blob/master/stats/demo.ipynb

--Correlation -- to understand if there is an influence of one variable on another. For example, experience and salary
p value -- randomness in relationship -- the much lesser, the much better

coefficient -- high coefficient is a strong relationship
End - 12th April


--Start - 14April
Linear Regression
Slope is very important. 
Price = intercept + slope * size 
y=a+bX
y indicates dependent variable. x is independent variable.
Intercept is Y value when x = 0 
Example -- cab charges -- initial intercept(base price) + km * price per km

If there are more than 1 variable -- for e.g. Size, bedrooms , age

5000 per sq/ft
-100,000 for age
200,000 for additional bedroom

There is only intercept but multiple slopes
y = a + (b1 * x1) + (b2 * x2) + (b3 * x3)
Example
price = 0 + (100 * 5000) + (5  * -100000) + (3 * 200,000)

Error -- distance between the prediction and actual is called error or Residual
The goal is to adjust the slope in such a way that it is close to reality

Mean Squared Error -- MSE -- to remove sign and

Sum (MSE)/n

-- End of 14 April

-- Start of 15 April
a = np.eye(5,5) -- eye -- identity matrix -- 1 is filled in diagonal

np.linspace -- linearspace
-- End of 15 April

-- Start 16th April
Slicing is a concept in python. Spend some time understandig slicing in python

Assignment -- Mini
Take an array of 5 x 5. Fill it with random numbers and display numbers that are greater than the average of the whole array.

Python for data analysis Wes McKinney, covers Panda's and Numpy
Python Data Science Handbook Jake Vanderplas
-- End 16th April

-- Start 17-April -- Methods of Array -- Methods of ndarray
a.mean() -- is called ndarray methid
print(np.add(a,b)) -- Universal function -- it is not a method -- because u r calling it with numpy library and not method of an object. 
Method is always a function of an object and hence object.method is how it is usually called.

What if u want to sort the whole 2 dimension array? -- Assignment

numpy.org -- for full numpy documentation

-- End 17th April

-- Start 19th April - Monday --- Copy vs View

-- Start 20th April
Coefficient -- coeff.T -- transpose

X.dot(coeff). Addint 1 as one of the features and coefficient at the beginning is imp

Broadcasting -- Smaller array is broadcast to larger array to perform operations


Assignment
How many pixels have more than 100 for red? 
If red is more than 100, reduce it to 100.

-- End 20th April

-- Start -- 21st April 
Pandas -- Series and Datframes
Series is a function
Index in Series is also known as rowlabel
Index can be nonnumeric and non-unique

Elements in a series can be accessed by position also. For e,g - s[0]
Error will be returned in case the custom index is also numeric and the position given is not defined as part of the custom numeric index. To overcome use the properties loc and iloc
loc is purely based on custom index. iloc is based on position index		


In dictionary -- key is unique
In series -- index can be duplicated

Display values in a series where there is a decrease from a previous value.

ndarray means numpy array
-- End 21st April
-- Start 22nd April
Important powerful methods
--apply
marks.apply(get_grade)
equivalent function in python is map 
map(get_grade, arks) but you need to write a for loop for printing
for v in map(get_grade,marks):
print(v) 

Slicing is important topic

You cant use loc for sorting and displaying top 2.
Because the index keeps changing. loc is position based on position and hence will display in correct values
for e.g. narks.sort_values(ascending=False).loc[:3] will all values until position 3 which is all values
For index based retrieval -- use loc
For position based retrieval -- use iloc

Fill nulls with values is called Imputing the values

When ever we use [] or loc[] it uses index
iloc[] will use position
pandas.pydata.org
when u get info from data frame -- if data type is object -- it means string
df.info()
-- End 22nd April

-- Start 23rd April
Retrieiving data from data frames

-- Normal Slicing, LOC and ILOC
at, iat, loc, icloc
at and iat do not support slicing
at -- only for specific row and column and not for slicing

e.g. print(df.at[0,'marks']) -- # 1st row -- marks column value
print(df.iat[0,2]) -- #1st row, 2nd column value

df.head() -- first 5
df.tail() -- last 5
df.sample(5) -- 5 random rows
df.subject.isin['Python','Java'] -- compare with a alist of values
Exercise -- Subject not java , marks less than 50

df.where(df.marks > 80)

df.filter -- filter is for labels. Not for data but for row or column labels
df.nlargest(4,'Marks') -- pick rows with first 4 highest marks

df['name'].drop_duplicates gives an array
df['name'].unique gives series
seaborn library
-- End 23-April

-- Start 24 April
df.apply  can be used to find out not null values fy axis = 1ws. If rows are required, s[pecify axis = 1
df.applymap() -- the function is applied to all rows and columns
df.name.upper() -- retirns error - upper cannot be used on series
instead use df.name.str.upper()
KM58 VDK

tRUE MEANS FALSE, PASS MEANS FAIL
ASSIGNMENT -- Print Pass or Fail not True or False

df.drop([11,13]) will remove rows but for a new copy
df.drop([12,13],inplace=True) will remove rows from the original
axis=0 is rows. depends on function.

Assignment
Create a new csv file books, movies or something else. 10rows
-- End 24 April

-- Start 26 April
df.remane(columns = {'rollno': 'admno'}
Concatenation, Joining
pd.concate((df1,df2), ignore_index=True)

If ignore_index=True is not set, then the indexes of the original data frames are takenn

joining in data frames is by default outer index.
The inner join is based on index

pd.concat
pd.merge -- merge , merges data based on common column
df1.merge(df2,how='oter') for outer join

Grouping
df.groupby('subject') -- # data frame group by object
grouping is for summaries 

subgroup = df.groupby('subject')

use tips data to see grouping happenin on multiple numeric columns

tips.groupby('sex')[[total_bill','tip']].mean()
grouping on multiple columns is possible
pandas is not just for data science. It is also primarily
for data analysis
-- End 26 April
-- Start 27 April

Data Plotting, Wrangling and Pivot Table

summary = tips.groupby(['day','time']).index
summary = tips.groupby(['day','time']),as_index=False).sum()

get_dummies()

pd.get_dummies(tips[['smoker','day']])
also called as 1 heart/or encoding
tips.pivot_table(values = 'total_bill', index = 'day', columns='sex',aggfunc=['sum','mean'])

Binning
tips['bill_bin']=pd.cut(tips.total_bill,5)
tips[['total_bill','bill_bin']]
Binning is useful when we have outliers
Using Log is another method of removing the effect of outliers so that ML algorithms can work correctly

pd.qcut(tips['total_bill'],[0,0.25,0.75,1], labels['LQ', 'UQ','IQR'])

pivot_table.plot.bar()

latest payslip
latest bank
credit report

techmyfile
-- End 27th April

-- Start 28th April
Matplotlib is a python 2d plotting library. There is an extension to draw 3d also.

-- End 28th April 

-- Start 29th April

-- End 29th April

--Start 30 April
Swarm Plot
Categorical Plots

Distance between the point and the line is called residual
If residual is on 0, then there is a difference between predict
and actual
Heat Map 
Correlation Matrix

seaborn is better than matplotlib

--End 30 April

--04-May- Start
pickling
deployment
zhango framework
Data Science Life Cycle
Feature Engg is used in Exploratory Data Analyis
Not all features need to be used for prediction.
In this phase , you can drop certain features after analysis
--04-MAy End

--05 May Start
Classification case study -- loan_prediction.ipynb
train = pd.read("loan.csv") -- train is the data frame
train.shape() will give you rows and columns 
outout -- 623,13
train.sample(5) will give a sample data of 5 rows
train.info() -- gives info about the data -- not null etc
train.describe() more info
train[['ApplicantIncome','CoapplicantIncome']] subset of values
train
-- 05 May End

--08-May-Start
different models
--08-May-End

-- 10-May Start
Train , Validation and Test data sets in the range of 60%, 20%, 20%

Cross validation will divide the data into K-Fold . If k =4 it will split into 4 sets. For each data set, a model is created.
The advantage is -- every observation is used in training and every observation is used in testing

model = 
scores = cross_val_score(model, train, y, cv = 5)

some of the algorithms use cross validation

E.g. LogisticRegressionCV
train_test_split?

confusion matrix gives an array of true positives and true negatives
Amber
--> Ensemble Methods
Most of the competitions were won by individuals who used Ensemble algorithms

load_iris from sklearn
iris data is toy data
from sklearn.datasets import load_iris

d= load_iris()
print(d.DESCR)

X, y = load_iris(return_X_y=True)
votingclassifier.ipynb
page 68 of course material
Bagging means --  Bootstrap Aggregation
We build the same model but it will take different set of training observations
Again Model voting is done
ensemble_models.ipynb

Genie Index
--10 May End

--Start 11-May
Technique Boosting
XGBoost -- in competitions if u want to win
Gradient boosting classifier which internally uses different decision trees
Stacking
Voting classifier is different

svc and knn require scaling

test_model.ipynb -- plots and curves -- advanced classification concepts

u have to use the same tfidf

---Sentiment Analysis

Bag of words
2 types of matrices
sparse matrix, dense matrix
hotrl_reviews.ipynb
--End 11-May

-- Start 12-May
Case Study 2
car/prepare_data.ipynb
gradient decent ,  regularisation, pipeline, how to create one, Efficient ways to check multple hyper parameters. Grid search
use case

df.isin([?]).sum() -- gives counts of values in each column which have ?
Finding the price of a car
price_finder_models.ipynb

-- End 12-May
--> 14- May Start
-- Parameters are values that the model generates 
-- Hyper parameters are for e.g. k in k nearest neighbours which are used to 
--Grid search is to find best hyper parameters
--Gradient decent is to find best parameters
is sklearn -- random_state = (1 or 12 0r 100 or any value) if we have to keep random state consitent
blob_demo
matplot lib is very important for scatter plots

blob_demo.ipynb for un supevised learning
for k means - always scale data
make_moons.ipynb
DBSCAN - density based clustering -- another technique
In dbscan -- u dont mention how many clusters are required. The algorithm figures it out
-- 14 May End

--Start 15-May
Page  101
DBSCAN
eps - epsilon or radius
customer_seg.ipynb
what should be the ideal epsilon -- radius? reducing epsilon will increase clusters and increasing will reduce them

Hierarchial clustering - Aglomerative -- suitable for small clusters
, Divisive -- suitable for large clusters
cluster map
seaborn.clustermap
dendrogram using scipy
ward is a function to calc distance

kmeans -- centroid -- dont place centroids randomly

unsupervised - Association - REcommender systems
REcommender system is an example of unsupervised machine learning using association
candidate generation
-- reduces billions down to hundreds of thousands
Scoring

--Re-ranking
Recommender - system -- content based and collaborative filtering

scoring
Assignment -- Display all border points in a different colour

implicit

Recommender System - Collaborative filtering
Predict user ratings for products based on a user's similarity with other users
It may be user based or item based
item based - neighbours

-- End - 15 May
-- Start - 16May
Recommending movies in collaborative filtering
movies_cf_final.ipynb
-- go through sides and understand clustering
